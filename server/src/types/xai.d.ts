export interface XAICompletionMessage {
  role: "system" | "user" | "assistant";
  content: string;
}

interface XAICompletionBaseParams {
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, 
   * decreasing the model's likelihood to repeat the same line verbatim.
   */
  frenquency_penalty?: number;
  /**
   * A JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. 
   * Mathematically, the bias is added to the logits generated by the model prior to sampling. 
   * The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; 
   * values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
   */
  logit_bias?: Object;
  /**
   * Whether to return log probabilities of the output tokens or not. 
   * If true, returns the log probabilities of each output token returned in the content of message.
   */
  logprobs?: boolean;
  /**
   * The maximum number of tokens that can be generated in the chat completion. 
   * This value can be used to control costs for text generated via API.
   */
  max_tokens?: number;
  /**
   * How many chat completion choices to generate for each input message. 
   * Note that you will be charged based on the number of generated tokens across all of the choices. 
   * Keep n as 1 to minimize costs.
   */
  n?: number;
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, 
   * increasing the model's likelihood to talk about new topics.
   */
  presence_penalty?: number;
  /**
   * If specified, our system will make a best effort to sample deterministically, 
   * such that repeated requests with the same `seed` and parameters should return the same result. 
   * Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend. 
   */
  seed?: number;
  /**
   * Up to 4 sequences where the API will stop generating further tokens.
   */
  stop?: Array<unknown>;
  /**
   * if set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, 
   * with the stream terminated by a `data: [DONE]` message.
   */
  stream?: boolean;
  stream_options?: unknown;
  /**
   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, 
   * while lower values like 0.2 will make it more focused and deterministic.
   */
  temperature?: number;
  /**
   * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
   * It is generally recommended to alter this or `temperature` but not both.
   */
  top_p?: number;
  /**
   * A unique identifier representing your end-user, which can help xAI to monitor and detect abuse.
   */
  user?: string;
}


/**
 * used for request for response for a given chat conversation
 */
export interface XAIChatCompletionParams implements XAICompletionBaseParams{
  /** 
   * A list of messages that make up the the chat conversation. 
   * Different models support different message types, such as image and text.
  */
  messages: XAICompletionMessage[];
  /**
   * Model name for the model to use.
   */
  model: string;
  
  response_format?: unknown;

  tool_choice?: unknown;

  tools?: Array<unknown>;
  /**
   * An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. 
   * logprobs must be set to true if this parameter is used.
   */
  top_logprobs?: number;
}

/**
 * used for request for response for a given prompt
 */
export interface XAICompletionParams implements XAICompletionBaseParams{
  /**
   * Specifies the model to be used for the request.
   */
  model: string;
  /**
   * prompt 
   */
  prompt:unknown;
  /**
   * Generates multiple completions internally and returns the top-scoring one. Not functional yet.
   */
  best_of?: number; 
  /**
   * Option to include the original prompt in the response along with the generated completion.
   */
  echo?: boolean;
}

export interface XAICompletionChoice {
  finish_reason: string;
  index: number;
  message: XAICompletionMessage;
}

export interface XAIUsage {
  completion_tokens: number;
  prompt_tokens: number;
  total_tokens: number;
}

export interface XAICompletionResponse {
  choices: XAICompletionChoice[];
  created: number;
  id: string;
  model: string;
  object: string;
  system_fingerprint: string;
  usage: XAIUsage;
}
